First I ran a count of the tag types and how many of each one there were using the following bit of code.
1.	with open(seattleOSM, 'r') as f:  
2.	      
3.	    tagDict = {}  
4.	    subTags = {}  
5.	    maxLat = 0  
6.	    maxLon = 0  
7.	      
8.	    for event, elem in ET.iterparse(f):  
9.	        if elem.tag in tagDict.keys():  
10.	            tagDict[elem.tag]['number'] += 1  
11.	            for attribute in elem.attrib:  
12.	                if attribute in tagDict[elem.tag].keys():  
13.	                    tagDict[elem.tag][attribute]+=1  
14.	                else:  
15.	                    tagDict[elem.tag][attribute] = 1  
16.	        else:  
17.	            tagNumber = {'number':1}  
18.	            tagDict[elem.tag] = tagNumber  
19.	            for attribute in elem.attrib:  
20.	                if attribute in tagDict[elem.tag].keys():  
21.	                    tagDict[elem.tag][attribute]+=1  
22.	                else:  
23.	                    tagDict[elem.tag][attribute] = 1  
24.	        if elem.tag=='bounds':  
25.	            print elem.attrib  
26.	        elif elem.tag=='osm':  
27.	            print elem.attrib  
28.	        elem.clear  
29.	    pprint.pprint(tagDict)  
















This produced the following results: 
tag Name	Number
bounds	1
member	40629
nd	7320270
node	6620098
osm	1
relation	657
tag	4295818
way	632414




The first problem that I am running into with this project is the size of the osm file for the city that I chose. I chose to explore the openstreetmap dataset for Seattle, Washington which was 1.48 GB. This posed a problem because it took a very long time to run code that combed through the dataset. 

When cleaning the data before importing into mongodb I ran into several issues just with making the street names uniform. To help with auditing the street names I modified the audit_street_names() function to add up the number of each street type found. The most glaring issues that I saw were things like FIXME and http://local.safeway.com/wa/tacoma-1594.html. I don’t think that we are quite at the point where we are using urls for physical street addresses yet. I also found things that just didn’t feel like they could be a real street type, an example would be bjuin. After googling this name I could find no supporting evidence that it is an actual street type in Seattle. There were also several city names that showed up as street types such as Cleveland, Milwaukee, and Snohomish (A suburb of seattle known for a mill). To investigate these strange street types I modified the expected street types to include more reasonable types that were not already in the list such as Bend, Vista, and Loop. I also made a separate list of possible street types and a list of very strange street names that seemed impossible to investigate further. This part of the auditing was done mostly subjectively. I also ran into problems like this

Name:  76th Avenue Northeast  Mapping:  {'Ave': 'Avenue', 'St.': 'Street', 'Rd.': 'Road', 'St': 'Street'} 76th Avenue Northeast => 76th Avenue

where the last part of the street name was cutoff, in this case the ‘Northeast’. To remedy the issue of the direction( for example Northeast) getting cut off I added all the possible directions to an expectedDirections list.

To mark which nodes with a street name were kept the same, edited, flagged as being suspicious or blatantly inaccurate I added a field to the JSON document that would be inserted to mongo. The codes for this field were 1,2,3,4 respectively. This allows mongo aggregation querys to be run to determine if there were any common themes with the inaccurate data. Perhaps there is a rogue contributor adding inaccurate data in an act of sabotage or there is a portion of the city which is more poorly mapped than the rest. Adding this field also allowed for the standardized data to be analyzed further without discarding the bad data. I think that in the case of a project like openstreetmaps where the community building the dataset there is a lot to be learned from the inconsistencies. One thing about this process of cleaning street names that may have introduced even more inconsistencies is that I manually went through the street types to copy and past them to their respective category and I also manually added all of the street name abbreviation mappings. The mappings probably could have been done programmatically with some kind of a regex but that is time consuming for me and I am not certain that such a thing wouldn’t end up introducing more errors that typing the mappings manually.  An issue I ran into with the mappings was that the update_name function began to recognize parts of street names as abbreviations thereby adding full street suffixes where they don’t belong.

After adding the Seattle data to mongodb I ran a few queries to get a feel for the contents of the data. 
